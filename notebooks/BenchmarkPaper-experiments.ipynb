{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark paper experiments \n",
    "\n",
    "This notebook contains the code for generating the pictures and graphs for the paper \"Waterberry Farms: a realistic benchmark for multi-robot informative path planning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow imports from the main source directory\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "#logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pathlib\n",
    "import pickle\n",
    "import gzip as compress\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "#import bz2 as compress\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "\n",
    "from InformationModel import StoredObservationIM\n",
    "from WaterberryFarm import create_wbfe, WaterberryFarm, MiniberryFarm, WaterberryFarmInformationModel, waterberry_score, get_datadir\n",
    "from WbfExperiment import menu\n",
    "\n",
    "#logging.basicConfig(level=logging.WARNING)\n",
    "#logging.getLogger().setLevel(logging.INFO)\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# all the experiments results etc. should go into this directory\n",
    "benchmark_dir = pathlib.Path(get_datadir(), \"Benchmark-Paper\")\n",
    "benchmark_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precompute environments\n",
    "* Precompute the evaluation of the environments. This allows the following simulations to run faster and to use the same environment for different path planning algorithms. \n",
    "* The precomputed data is saved into the corresponding directory under __Temporary/2022-01-15-MREM_data. __Delete that directory if you want to run the simulation with a differently parameterized environments.__\n",
    "* This precalculates 50 days of evolution for all four environments. This is sufficient for the standard benchmark settings. \n",
    "* As this simulation is expensive for the large environments, be patient, it takes about __30 minutes__ on a fast machine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute = 50\n",
    "# typenames = [\"Miniberry-10\", \"Miniberry-30\", \"Miniberry-100\", \"Waterberry\"]\n",
    "typenames = [\"Miniberry-10\",\"Miniberry-30\", \"Miniberry-100\"]\n",
    "time_start_environment = 6\n",
    "\n",
    "# typenames = [\"Waterberry\"]\n",
    "# typenames = [\"Miniberry-100\"]\n",
    "for typename in typenames:\n",
    "    menu({\"geometry\": typename, \"action\": \"precompute-environment\", \"precompute-time\": precompute})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate pictures of the geometry. \n",
    "Generates pictures for the geometry of the various benchmark configurations (Miniberry-10,30,100 and Waterberry), and saves them into files $data/geometry_Miniberry-10.pdf etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = [1, 3, 5, 10]\n",
    "# typename = \"Miniberry-10\"\n",
    "\n",
    "for typename in typenames:\n",
    "    _, wbfe, _ = create_wbfe(saved = True, wbf_prec = None, typename = typename)\n",
    "    geom = wbfe.geometry\n",
    "    fig, ax = plt.subplots(1, figsize=(5,4))\n",
    "    wbfe.geometry.visualize(ax)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(pathlib.Path(benchmark_dir, f\"geometry_{typename}.pdf\"))\n",
    "    plt.savefig(pathlib.Path(benchmark_dir, f\"geometry_{typename}.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One day, multi-value\n",
    "Performing a set of experiments in the one day multi-value setting for a number of sample policies. \n",
    "First we create some helper functions to create the kind of graphs we are interested in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_of_day_graphs(results, graphfilename = \"EndOfDayGraph.pdf\", title = None):\n",
    "    \"\"\"From the results of a 1 day experiment, create a figure that shows the\n",
    "    environment, the information model at the end of the scenario, the path of the robot and the evolution of the score\"\"\"\n",
    "    #print(results)\n",
    "    wbfe = results[\"wbfe\"]\n",
    "    wbfim = results[\"wbfim\"]\n",
    "    fig, ((ax_robot_path, ax_env_tylcv, ax_im_tylcv, ax_env_ccr, ax_im_ccr,\n",
    "    ax_env_soil, ax_im_soil, ax_scores)) = plt.subplots(1, 8, figsize=(18,3))\n",
    "    if title is None:\n",
    "        fig.suptitle(results[\"policy\"], fontsize=16)\n",
    "    elif title != \"\":\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # visualize the observations, which gives us the path of the robot\n",
    "    empty = np.ones_like(wbfe.tylcv.value.T)\n",
    "    image_env_tylcv = ax_robot_path.imshow(empty, vmin=0, vmax=1, origin=\"lower\", cmap=\"gray\")    \n",
    "    ax_robot_path.set_title(\"Robot path\")\n",
    "\n",
    "    obsx = []\n",
    "    obsy = []\n",
    "    for obs in results[\"observations\"]:\n",
    "        obsx.append(obs[StoredObservationIM.X])\n",
    "        obsy.append(obs[StoredObservationIM.Y])\n",
    "        old_obs = obs\n",
    "    ax_robot_path.add_line(lines.Line2D(obsx, obsy, color=\"red\"))\n",
    "\n",
    "    # visualize the environment for tylcv\n",
    "    image_env_tylcv = ax_env_tylcv.imshow(wbfe.tylcv.value.T, vmin=0, vmax=1, origin=\"lower\", cmap=\"gray\")\n",
    "    ax_env_tylcv.set_title(\"TYLCV Env.\")\n",
    "    # visualize the information model for tylcv\n",
    "    image_im_tylcv = ax_im_tylcv.imshow(wbfim.im_tylcv.value.T, vmin=0, vmax=1, origin=\"lower\", cmap=\"gray\")\n",
    "    ax_im_tylcv.set_title(\"TYLCV Estimate\")\n",
    "\n",
    "    # visualize the environment for ccr\n",
    "    image_env_ccr = ax_env_ccr.imshow(wbfe.ccr.value.T, vmin=0, vmax=1, origin=\"lower\", cmap=\"gray\")\n",
    "    ax_env_ccr.set_title(\"CCR Env\")\n",
    "    # visualize the information model for ccr\n",
    "    image_im_ccr = ax_im_ccr.imshow(wbfim.im_ccr.value.T, vmin=0, vmax=1, origin=\"lower\", cmap=\"gray\")\n",
    "    ax_im_ccr.set_title(\"CCR Estimate\")\n",
    "\n",
    "    # visualize the environment for soil humidity\n",
    "    image_env_soil = ax_env_soil.imshow(wbfe.soil.value.T, vmin=0, vmax=1, origin=\"lower\", cmap=\"gray\")\n",
    "    ax_env_soil.set_title(\"Soil Humidity Env.\")\n",
    "    # visualize the information model for soil humidity\n",
    "    image_im_soil = ax_im_soil.imshow(wbfim.im_soil.value.T, vmin=0, vmax=1, origin=\"lower\", cmap=\"gray\")\n",
    "    ax_im_soil.set_title(\"Soil Humidity Est.\")\n",
    "\n",
    "    ax_scores.plot(results[\"scores\"])\n",
    "    ax_scores.set_ylim(top=0)\n",
    "    ax_scores.set_xlabel(\"Time\")\n",
    "    # ax_scores.set_ylabel(\"Score\")\n",
    "    ax_scores.set_title(\"Scores\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(pathlib.Path(benchmark_dir, graphfilename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare different policies in a one-day experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_choices = {\"geometry\": \"Miniberry-30\", \"action\": \"run\", \"scenario\": \"one-day-single-value\", \"visualize\": 0,  \"result-basedir\": benchmark_dir}\n",
    "default_choices[\"velocity\"] = 1\n",
    "default_choices[\"timesteps_per_day\"] = 500 # 1000\n",
    "default_choices[\"time_start_environment\"] = 6\n",
    "\n",
    "# if force_run is false, we do not re-run an already run policy \n",
    "force_run = False\n",
    "policies = [\"benchmarkpaper-randomwaypoint\", \n",
    "\"benchmarkpaper-lawnmower\", \"benchmarkpaper-spiral\", \"benchmarkpaper-adaptive-lawnmower\"]\n",
    "for policy in policies:\n",
    "    print(policy)\n",
    "    # do a dryrun, to get the path to the cached values\n",
    "    choices = copy.copy(default_choices)\n",
    "    choices[\"policy\"] = policy\n",
    "    choices[\"dryrun\"] = True\n",
    "    results = menu(choices)\n",
    "    path = results[\"results_path\"]\n",
    "    print(path)\n",
    "    # avoid re-running experiments\n",
    "    if not path.exists() or force_run:\n",
    "        choices[\"dryrun\"] = False\n",
    "        results = menu(choices)\n",
    "    else:\n",
    "        with compress.open(path, \"rb\") as f:\n",
    "            results = pickle.load(f)\n",
    "    end_of_day_graphs(results, f\"daily-summary-{results['policy']}-{results['estimator']}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare estimators in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_choices = {\"geometry\": \"Miniberry-30\", \"action\": \"run\", \"scenario\": \"one-day-single-value\", \"visualize\": 0,  \"result-basedir\": benchmark_dir}\n",
    "default_choices[\"velocity\"] = 1\n",
    "default_choices[\"timesteps_per_day\"] = 500 # 1000\n",
    "default_choices[\"time_start_environment\"] = 6\n",
    "default_choices[\"policy\"] = \"benchmarkpaper-randomwaypoint\"\n",
    "\n",
    "\n",
    "# if force_run is false, we do not re-run an already run policy \n",
    "force_run = False\n",
    "for estimator in [\"AD\", \"GP\"]:\n",
    "    print(policy)\n",
    "    # do a dryrun, to get the path to the cached values\n",
    "    choices = copy.copy(default_choices)\n",
    "    choices[\"estimator\"]  = estimator\n",
    "    choices[\"dryrun\"] = True\n",
    "    results = menu(choices)\n",
    "    path = results[\"results_path\"]\n",
    "    print(path)\n",
    "    # avoid re-running experiments\n",
    "    if not path.exists() or force_run:\n",
    "        choices[\"dryrun\"] = False\n",
    "        results = menu(choices)\n",
    "    else:\n",
    "        with compress.open(path, \"rb\") as f:\n",
    "            results = pickle.load(f)\n",
    "    end_of_day_graphs(results, f\"daily-summary-{results['policy']}-{results['estimator']}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance of various combinations of path planner and estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all the results. As I am reading them in, generate the graphs.\n",
    "allresults = []\n",
    "for a in benchmark_dir.iterdir():\n",
    "    if a.name.startswith(\"res-pol_benchmarkpaper-\"):\n",
    "        print(a)\n",
    "        with compress.open(a, \"rb\") as f:\n",
    "            results = pickle.load(f)\n",
    "            # end_of_day_graphs(results, f\"daily-summary-{results['policy']}.pdf\")\n",
    "            allresults.append(results)\n",
    "\n",
    "fig, ax_scores = plt.subplots(1, figsize=(6,4))\n",
    "for results in allresults:\n",
    "    ax_scores.plot(results[\"scores\"], label = f'{results[\"policy\"][len(\"benchmarkpaper-\"):]}+{results[\"estimator\"]}')\n",
    "ax_scores.set_ylim(top=0)\n",
    "ax_scores.set_xlabel(\"Time\")\n",
    "ax_scores.set_ylabel(\"Score\")\n",
    "ax_scores.set_title(\"Scores\")\n",
    "ax_scores.legend()\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the computational cost of different estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "file = pathlib.Path(benchmark_dir, \"timecomp\")\n",
    "\n",
    "if file.exists():\n",
    "    with open(file, \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "else:\n",
    "    res = {}\n",
    "    for estimator in [\"AD\", \"GP\"]:\n",
    "        for typename in typenames:\n",
    "            wbf, wbfe, _ = create_wbfe(saved = True, wbf_prec = None, typename = typename)\n",
    "            wbfe.proceed(time_start_environment)\n",
    "            wbfim = WaterberryFarmInformationModel(\"wbfi\", wbf.width, wbf.height, estimator=\"GP\")\n",
    "            ## create some observations and add them...\n",
    "\n",
    "            times = []\n",
    "            for time in range(200):\n",
    "                position = [10, 10, 7]\n",
    "                # print(results[\"robot\"])\n",
    "                #positions.append(position)\n",
    "                obs = wbfe.get_observation(position)\n",
    "                # observations.append(obs)\n",
    "                wbfim.add_observation(obs)\n",
    "                ## measure the time it takes here\n",
    "                time = timeit.timeit(\"wbfim.proceed(1)\", number=1,  globals=globals())\n",
    "                print(time)\n",
    "                times.append(time)\n",
    "                #wbfim.proceed(1)\n",
    "            res[f\"{typename}_{estimator}\"] = times\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
    "\n",
    "for key in res:\n",
    "    ax.plot(res[key], label = key)\n",
    "#ax.set_ylim(top=0)\n",
    "ax.set_xlabel(\"Observations\")\n",
    "ax.set_ylabel(\"Time\")\n",
    "#ax_scores.set_title(\"Scores\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.savefig(pathlib.Path(benchmark_dir, \"estimator_computational_cost.pdf\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution of the environments at a list of specific timepoints\n",
    "\n",
    "Generates figures for the state of the environment components at various timepoints. The default timepoints are the 1, 5, 10, 15 that are good for visualizing what goes on in the multiday experiments. \n",
    "\n",
    "The results are saved into $data/env-dynamics-{typename}-{value} files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typenames = [\"Miniberry-10\", \"Miniberry-30\", \"Miniberry-100\", \"Waterberry\"]\n",
    "values = [\"tylcv\", \"ccr\", \"soil\"]\n",
    "timepoints = [1, 5, 10, 15]\n",
    "\n",
    "for value in values:\n",
    "    for typename in typenames:\n",
    "        _, wbfe, _ = create_wbfe(saved = True, wbf_prec = None, typename = typename)\n",
    "        fig, axs = plt.subplots(1, len(timepoints), figsize = (4 * len(timepoints), 4))\n",
    "        fig.suptitle(f\"{typename} / {value}\")\n",
    "        ntt = 0\n",
    "        for t in range(timepoints[-1]+1):\n",
    "            if t >= timepoints[ntt]:\n",
    "                if value == \"tylcv\":\n",
    "                    val = wbfe.tylcv.value\n",
    "                elif value == \"ccr\":\n",
    "                    val = wbfe.ccr.value\n",
    "                elif value == \"soil\":\n",
    "                    val = wbfe.soil.value\n",
    "                axs[ntt].imshow(val.T, vmin=0, vmax=1, cmap=\"gray\", origin=\"lower\")\n",
    "                axs[ntt].set_title(f\"t={timepoints[ntt]}\")\n",
    "                ntt = ntt + 1\n",
    "            wbfe.proceed()\n",
    "        fig.tight_layout()\n",
    "        plt.savefig(pathlib.Path(benchmark_dir, f\"env-dynamics-{typename}-{value}.pdf\"))\n",
    "        plt.savefig(pathlib.Path(benchmark_dir, f\"env-dynamics-{typename}-{value}.jpg\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single day, single robot, multi-value experiment for a given policy\n",
    "* The results will be saved into `2022-01-15-MREM_data/Miniberry-10/res_pol#`\n",
    "    * The # is the policy number, see in \n",
    "* The policy # followed by the robot is determined in the WbfExperiment.menu function, search for ROBOT AND POLICY SPECIFICATION\n",
    "* In its current form, this call forces the experiment to be run and it ends up writing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = {}\n",
    "\n",
    "results = menu({\"geometry\": 1, \"action\": 4, \"scenario\": 1, \"visualize\": 0, \"time_start_environment\": 6, \"policy\": 1, \"policy_description\": desc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
