{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import random\n",
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "# , WhiteKernel, RationalQuadratic, ExpSineSquared\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "from Environment import Environment, PollutionModelEnvironment, EpidemicSpreadEnvironment\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformationModel:\n",
    "    \"\"\"The ancestor of all information models. This defines the functions we can \n",
    "    call on these. They will need to be specialized to the different information models\"\"\"\n",
    "\n",
    "    def __init__(self, name, width, height):\n",
    "        self.name, self.width, self.height = name, width, height\n",
    "    \n",
    "    def score(self, env: Environment):\n",
    "        \"\"\"Calculates a score that estimates the quality of this information model in modeling the\n",
    "        specified environment\"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def add_observation(self, observation: dict):\n",
    "        \"\"\"Adds an observation as a dictionary with the fields value, x, y, timestamp etc. Different \n",
    "        implementations do different things with these observations (store, use it right away \n",
    "        to update the model etc.)\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def proceed(self, delta_t: float):\n",
    "        \"\"\"Proceed with the information model. The general assumption here is that after calling this\n",
    "        the estimates are pre-computed and ready to be queried. Some implementations might model the\n",
    "        evolution of the system as well.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarFieldInformationModel_stored_observation(InformationModel):\n",
    "    \"\"\"An information model for scalar fields. It receives a series of observations, . It stores all the observations, and then uses them at estimate \n",
    "    time. This version is just keeping stored observations.\"\"\"\n",
    "\n",
    "    def __init__(self, name, width, height, estimation_type = \"point\", estimation_radius = 5):\n",
    "        \"\"\"Initializes the value to zero, the uncertainty to one. \n",
    "        FIXME: what exactly the uncertainty measures??? \"\"\"\n",
    "        super().__init__(name, width, height)\n",
    "        self.observations = []\n",
    "        self.estimation_type = estimation_type\n",
    "        self.estimation_radius = estimation_radius \n",
    "    \n",
    "    def score(self, env):\n",
    "        \"\"\"Scores the information model by finding the difference from the environment. In this case we are\n",
    "        using models.\"\"\"\n",
    "        return np.sum(np.abs(env.value - self.value))\n",
    "    \n",
    "    def add_observation(self, observation):\n",
    "        \"\"\"The simplest way to add an observation is that we just record it.\"\"\"\n",
    "        self.observations.append(observation)\n",
    "        \n",
    "    def proceed(self, delta_t):\n",
    "        \"\"\"Proceeds a step in time. \n",
    "        At the current point, this basically performs an estimation, based on the information\n",
    "        \n",
    "        None of the currently used estimators \n",
    "        here \"\"\"\n",
    "        self.value, self.uncertainty = self.estimate(self.observations)\n",
    "        \n",
    "    def estimate(self, observations):\n",
    "        \"\"\" Performs the estimate for every point in the environment. Returns a pair\n",
    "        of the values and uncertainty as matrices for every point in the environment.\"\"\"\n",
    "        if self.estimation_type == \"point\":\n",
    "            return self.estimate_with_point(observations)\n",
    "        if self.estimation_type == \"disk-auto\":\n",
    "            return self.estimate_with_disk(observations)\n",
    "        if self.estimation_type == \"disk-fixed\":\n",
    "            return self.estimate_with_disk(observations, self.estimation_radius)\n",
    "        if self.estimation_type == \"gaussian-process\":\n",
    "            return self.estimate_with_gaussian_process(observations)\n",
    "        raise Exception(f\"Unknown estimation type {self.estimation_type}\")\n",
    "        \n",
    "        \n",
    "    def estimate_with_point(self, observations):\n",
    "        \"\"\"Takes all the observations and estimates the value and the uncertainty. This one processes all the \n",
    "        observations, ignores the timestamp, and assumes that each observation refers only to the current point.\"\"\"\n",
    "        # value = np.ones((self.width, self.height)) * 0.5\n",
    "        value = np.zeros((self.width, self.height)) \n",
    "        uncertainty = np.ones((self.width, self.height))\n",
    "        for obs in observations:\n",
    "            value[obs[\"x\"], obs[\"y\"]] = obs[\"value\"]\n",
    "            uncertainty[obs[\"x\"], obs[\"y\"]] = 0\n",
    "        return value, uncertainty\n",
    "\n",
    "    def estimate_with_disk(self, observations, radius=None):\n",
    "        \"\"\"Consider that we are estimating them with a disk of a certain radius r. We set the values to it.\n",
    "        The radius r can be dynamically calculated such that the total disks achieve 2x the coverage of the \n",
    "        area. sqrt((height * width * 2) / pi). \n",
    "        Then we set the values in a disk\"\"\"\n",
    "        value = np.zeros((self.width, self.height)) \n",
    "        uncertainty = np.ones((self.width, self.height))\n",
    "        if radius == None:\n",
    "            radius = 1+math.sqrt((self.height * self.width * 2) / (math.pi * len(observations)))\n",
    "        # FIXME: this is not very efficient, it can be made more efficient by iterating just one corner\n",
    "        # and finding values.\n",
    "        for obs in observations:\n",
    "            x = obs[\"x\"]\n",
    "            y = obs[\"y\"]\n",
    "            for i in range(round(max(0, x-radius)), round(min(x+radius, self.width))):\n",
    "                for j in range(round(max(0, y-radius)), round(min(y+radius, self.height))):\n",
    "                    if (math.sqrt((i-x)*(i-x) + (j-y)*(j-y)) <= radius):\n",
    "                        # FIXME: could do better... with averaging taking into consideration the i and j\n",
    "                        value[i, j] = obs[\"value\"]\n",
    "                        uncertainty[i, j] = 0\n",
    "        return value, uncertainty\n",
    "    \n",
    "    def estimate_with_gaussian_process(self, observations):\n",
    "        # calculate the estimate for each gaussian process\n",
    "        est = np.zeros([self.width,self.height])\n",
    "        stdmap = np.zeros([self.width,self.height])\n",
    "        if len(observations) == 0:\n",
    "            return est, stdmap\n",
    "        X = []\n",
    "        Y = []\n",
    "        for obs in observations:\n",
    "            # unclear if this rounding matters\n",
    "            X.append([round(obs[\"x\"]), round(obs[\"y\"])])\n",
    "            Y.append([obs[\"value\"]])\n",
    "        # fit the gaussian process\n",
    "        # fit the gaussian process\n",
    "        rbf = 2.0 * RBF(length_scale = [1.0, 1.0], length_scale_bounds = [1, 10])\n",
    "        # rbf = RBF(length_scale = [2.0, 2.0], length_scale_bounds = \"fixed\")\n",
    "        ## rbf = 2.0 * RBF(length_scale = [1.0, 1.0], length_scale_bounds = [1, 10])\n",
    "        # rbf = 2.0 * RBF(length_scale = [1.0, 1.0])\n",
    "        gpr = GaussianProcessRegressor(kernel=rbf)\n",
    "        gpr.fit(X,Y)\n",
    "        x = []\n",
    "        X = np.array(list(itertools.product(range(self.width), range(self.height))))\n",
    "        Y, std = gpr.predict(X, return_std = True)\n",
    "        for i, idx in enumerate(X):\n",
    "            est[idx[0], idx[1]] = Y[i]\n",
    "            stdmap[idx[0], idx[1]] = std[i]\n",
    "        # print(std.sum())\n",
    "        return est, stdmap\n",
    "    \n",
    "    def estimate_voi(self, observation):\n",
    "        \"\"\"The voi of the observation is the reduction of the uncertainty.\n",
    "        FIXME this can be made different for the GP\"\"\"\n",
    "        _, uncertainty = self.estimate(self.observations)\n",
    "        observations_new = observations.clone()\n",
    "        observations_new.append(observation)\n",
    "        _, uncertainty_new = self.estimate(observations_new)        \n",
    "        return np.sum(np.abs(uncertainty - uncertainty_new))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an environment to observe\n",
    "env = PollutionModelEnvironment(\"water\", 100, 100)\n",
    "env.evolve_speed = 1\n",
    "env.p_pollution = 0.1\n",
    "for t in range(120):\n",
    "    env.proceed()\n",
    "plt.imshow(env.value, vmin=0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an observation model\n",
    "#im = ScalarFieldInformationModel_stored_observation(\"sample\", env.width, env.height, \\\n",
    "#                                                    estimation_type=\"disk-fixed\", \n",
    "#                                                    estimation_radius=10)\n",
    "im = ScalarFieldInformationModel_stored_observation(\"sample\", env.width, env.height, \\\n",
    "                                                    estimation_type=\"gaussian-process\"\n",
    "                                                    )\n",
    "# generate a series random observations\n",
    "for i in range(30):\n",
    "    x = random.randint(0, env.width-1)\n",
    "    y = random.randint(0, env.height-1)\n",
    "    value = env.value[x,y]\n",
    "    obs = {\"x\": x, \"y\": y, \"value\": value}\n",
    "    im.add_observation(obs)\n",
    "im.proceed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im.value, vmin=0, vmax=1.0)\n",
    "#plt.imshow(im.uncertainty, vmin=0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im.uncertainty, vmin=0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b289c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
